/*@Atomic@*/ /*@CudaAtomic@*/ #include <cuda/atomic> /*@RaceBug@*/
/*@IntType@*/ typedef int data_type; /*@LongType@*/ typedef long long data_type;
/*@Atomic@*/ typedef int idx_type; /*@CudaAtomic@*/ typedef cuda::atomic<int> idx_type; /*@RaceBug@*/ typedef int idx_type;

static const int ThreadsPerBlock = 512;
//BlankLine

#include "indigo_mst_edge_cuda.h"
//BlankLine

/*@Atomic@*/ /*@CudaAtomic@*/ /*@RaceBug@*/ 
/*@NonPersist@*/ /*@Persist@*/ 
/*@NoExcessThreadsBug@*/ /*@ExcessThreadsBug@*/ /*@NoBoundsBug@*/ /*@BoundsBug@*/ 
/*@NoFieldBug@*/ /*@FieldBug@*/ 

/*@+Persist@*/
/*@NoExcessThreadsBug@*/ suppress /*@ExcessThreadsBug@*/ suppress /*@NoBoundsBug@*/ /*@BoundsBug@*/
/*@-Persist@*/

static __global__ void init(const ECLgraph g, data_type* const eweight, bool* const included, idx_type* const parent, idx_type* const minv)
{
  const int v = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  if (v < g.nodes) {
    //initially, each vertex is its own parent
    parent[v] = v;
    minv[v] = INT_MAX;
  }
  if (v < g.edges) {
    included[v] = false;
    eweight[v] = g.eweight[v];
  }
}
//BlankLine

static __device__ int findRep(int idx, idx_type* const parent)
{
  /*@Atomic@*/ int curr = atomicRead(&parent[idx]); /*@CudaAtomic@*/ int curr = parent[idx].load(); /*@RaceBug@*/ int curr = parent[idx];
  if (curr != idx) {
    int next, prev = idx;
    /*@Atomic@*/ while (curr != (next = atomicRead(&parent[curr]))) { /*@CudaAtomic@*/ while (curr != (next = parent[curr].load())) { /*@RaceBug@*/ while (curr != (next = parent[curr])) {
      /*@Atomic@*/ atomicWrite(&parent[prev], next); /*@CudaAtomic@*/ parent[prev].store(next); /*@RaceBug@*/ parent[prev] = next;
      prev = curr;
      curr = next;
    }
  }
  return curr;
}
//BlankLine

static __device__ void join(int arep, int brep, idx_type* const parent)
{
  int mrep;
  do {
    mrep = max(arep, brep);
    arep = min(arep, brep);
  /*@Atomic@*/ } while ((brep = atomicCAS(&parent[mrep], mrep, arep)) != mrep); /*@CudaAtomic@*/ } while ((brep = atomicCAS_CUDA(&parent[mrep], mrep, arep)) != mrep); /*@RaceBug@*/ } while ((brep = atomicCAS(&parent[mrep], mrep, arep)) != mrep);
}
//BlankLine

static __global__ void mst_first(const ECLgraph g, data_type* const eweight, const int* const sp, idx_type* const minv, idx_type* const parent, data_type* const goagain)
{
  /*@NoFieldBug@*/ const int N = g.edges; /*@FieldBug@*/ const int N = g.nodes;
  
  /*@+NonPersist@*/
  int e = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  /*@NoExcessThreadsBug@*/ if (e < N) { /*@ExcessThreadsBug@*/ { /*@NoBoundsBug@*/ suppress /*@BoundsBug@*/ suppress
  /*@-NonPersist@*/

  /*@+Persist@*/ 
  /*@NoExcessThreadsBug@*/ suppress /*@ExcessThreadsBug@*/ suppress /*@NoBoundsBug@*/ for (int e = threadIdx.x + blockIdx.x * ThreadsPerBlock; e < N; e += gridDim.x * ThreadsPerBlock) { /*@BoundsBug@*/ for (int e = threadIdx.x + blockIdx.x * ThreadsPerBlock; e <= N; e += gridDim.x * ThreadsPerBlock) { 
  /*@-Persist@*/
    
    const int src = sp[e];
    const int dst = g.nlist[e];
    //BlankLine
    
    if (dst > src) {
      const int arep = findRep(src, parent);
      const int brep = findRep(dst, parent);
      if (arep != brep) {
        /*@Atomic@*/ atomicWrite(goagain, 1); /*@CudaAtomic@*/ *goagain = 1; /*@RaceBug@*/ *goagain = 1;
        //BlankLine
      
        int edx, upd;
        /*@Atomic@*/ upd = atomicRead(&minv[arep]); /*@CudaAtomic@*/ upd = minv[arep].load(); /*@RaceBug@*/ upd = minv[arep];
        do {
          edx = upd;
          const data_type wei = (edx == INT_MAX) ? maxval : eweight[edx];
          if ((eweight[e] > wei) || ((eweight[e] == wei) && (e >= edx))) break;
        /*@Atomic@*/ } while ((upd = atomicCAS(&minv[arep], edx, e)) != edx); /*@CudaAtomic@*/ } while ((upd = atomicCAS_CUDA(&minv[arep], edx, e)) != edx); /*@RaceBug@*/ } while ((upd = atomicCAS(&minv[arep], edx, e)) != edx);
        //BlankLine
        
        /*@Atomic@*/ upd = atomicRead(&minv[brep]); /*@CudaAtomic@*/ upd = minv[brep].load(); /*@RaceBug@*/ upd = minv[brep];
        do {
          edx = upd;
          const data_type wei = (edx == INT_MAX) ? maxval : eweight[edx];
          if ((eweight[e] > wei) || ((eweight[e] == wei) && (e >= edx))) break;
        /*@Atomic@*/ } while ((upd = atomicCAS(&minv[brep], edx, e)) != edx); /*@CudaAtomic@*/ } while ((upd = atomicCAS_CUDA(&minv[brep], edx, e)) != edx); /*@RaceBug@*/ } while ((upd = atomicCAS(&minv[brep], edx, e)) != edx);
      }
    }
  }
}
//BlankLine

static __global__ void mst_second(const ECLgraph g, const int* const sp, bool* const included, idx_type* const minv, idx_type* const parent)
{
  /*@NoFieldBug@*/ const int N = g.edges; /*@FieldBug@*/ const int N = g.nodes;
  
  /*@+NonPersist@*/
  int e = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  /*@NoExcessThreadsBug@*/ if (e < N) { /*@ExcessThreadsBug@*/ { /*@NoBoundsBug@*/ suppress /*@BoundsBug@*/ suppress
  /*@-NonPersist@*/

  /*@+Persist@*/ 
  /*@NoExcessThreadsBug@*/ suppress /*@ExcessThreadsBug@*/ suppress /*@NoBoundsBug@*/ for (int e = threadIdx.x + blockIdx.x * ThreadsPerBlock; e < N; e += gridDim.x * ThreadsPerBlock) { /*@BoundsBug@*/ for (int e = threadIdx.x + blockIdx.x * ThreadsPerBlock; e <= N; e += gridDim.x * ThreadsPerBlock) { 
  /*@-Persist@*/

    const int src = sp[e];
    const int dst = g.nlist[e];
    //BlankLine
    
    if (dst > src) {
      const int arep = findRep(src, parent);
      const int brep = findRep(dst, parent);
      if (arep != brep) {
        if ((e == minv[arep]) || (e == minv[brep])) { 
        // if this was the best edge found for the set
          join(arep, brep, parent);
         
         included[e] = true;
        }
      }
    }
  }
}
//BlankLine

static double GPUmst(const ECLgraph g, const int* const sp, bool* const included)
{
  bool* d_included;
  if (cudaSuccess != cudaMalloc((void **)&d_included, g.edges * sizeof(bool))) fprintf(stderr, "ERROR: could not allocate d_included\n");
  
  idx_type* d_parent;
  if (cudaSuccess != cudaMalloc((void **)&d_parent, g.nodes * sizeof(idx_type))) fprintf(stderr, "ERROR: could not allocate d_parent\n");
  
  idx_type* d_minv;
  if (cudaSuccess != cudaMalloc((void **)&d_minv, g.nodes * sizeof(idx_type))) fprintf(stderr, "ERROR: could not allocate d_minv\n");
  
  data_type* d_eweight;
  if (cudaSuccess != cudaMalloc((void **)&d_eweight, g.edges * sizeof(data_type))) fprintf(stderr, "ERROR: could not allocate d_eweight\n");
  
  data_type* d_goagain;
  if (cudaSuccess != cudaMalloc((void **)&d_goagain, sizeof(data_type))) fprintf(stderr, "ERROR: could not allocate d_goagain\n");
  
  /*@+NonPersist@*/
  const int blocks = (g.edges + ThreadsPerBlock - 1) / ThreadsPerBlock; 
  /*@-NonPersist@*/
  /*@+Persist@*/
  const int ThreadsBound = GPUinfo(0, false);
  const int blocks = ThreadsBound / ThreadsPerBlock;
  /*@-Persist@*/
  //BlankLine
  
  init<<<(MAX(g.nodes, g.edges) + ThreadsPerBlock - 1) / ThreadsPerBlock, ThreadsPerBlock>>>(g, d_eweight, d_included, d_parent, d_minv);
  
  //BlankLine
  struct timeval start, end;
  gettimeofday(&start, NULL);
  //BlankLine
  
  data_type goagain;
  int iter = 0;
  do {
    iter++;
    cudaMemset(d_goagain, 0, sizeof(data_type));
    //BlankLine
    
    mst_first<<<blocks, ThreadsPerBlock>>>(g, d_eweight, sp, d_minv, d_parent, d_goagain);
    //BlankLine
    
    if (cudaSuccess != cudaMemcpy(&goagain, d_goagain, sizeof(data_type), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of goagain from device failed\n");
    if (goagain) {
      mst_second<<<blocks, ThreadsPerBlock>>>(g, sp, d_included, d_minv, d_parent);
      //BlankLine
      
      fill_darray<<<(g.nodes + ThreadsPerBlock - 1) / ThreadsPerBlock, ThreadsPerBlock>>>(d_minv, INT_MAX, g.nodes);
    }
  } while (goagain);
  //BlankLine
  
  cudaDeviceSynchronize();
  gettimeofday(&end, NULL);
  double runtime = end.tv_sec + end.tv_usec / 1000000.0 - start.tv_sec - start.tv_usec / 1000000.0;
  CheckCuda();
  printf("iterations: %d\n", iter);
  //BlankLine
  
  if (cudaSuccess != cudaMemcpy(included, d_included, g.edges * sizeof(bool), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of included from device failed\n");
  
  //BlankLine
  cudaFree(d_included);
  cudaFree(d_parent);
  cudaFree(d_minv);
  cudaFree(d_eweight);
  cudaFree(d_goagain);
  return runtime;
}
