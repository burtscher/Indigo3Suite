/*@push@*/ declare /*@pull@*/ declare
/*@NonDeterm@*/ declare /*@Determ@*/ declare
/*@IntType@*/ declare /*@LongType@*/ declare
/*@NonPersist@*/ declare /*@Persist@*/ declare
/*@Atomic@*/ declare /*@CudaAtomic@*/ #include <cuda/atomic> /*@RaceBug@*/ declare
/*@Thread@*/ declare /*@Warp@*/ declare /*@Block@*/ declare

/*@NoFieldBug@*/ declare /*@FieldBug@*/ declare

/*@+Persist@*/
/*@NoExcessThreadsBug@*/ suppress /*@ExcessThreadsBug@*/ suppress /*@NoBoundsBug@*/ /*@BoundsBug@*/ 
/*@-Persist@*/
/*@NoUninitializedBug@*/ declare /*@UninitializedBug@*/ declare

/*@push@*/ /*@pull@*/ suppress
/*@Thread@*/ /*@Warp@*/ suppress /*@Block@*/ suppress

/*@Atomic@*/ typedef int flag_t; /*@CudaAtomic@*/ typedef cuda::atomic<int> flag_t; /*@RaceBug@*/ typedef int flag_t;
/*@IntType@*/ typedef int data_type; /*@LongType@*/ typedef unsigned long long data_type;
typedef data_type basic_t;

static const int ThreadsPerBlock = 512;
//BlankLine

#include "indigo_mis_edge_cuda.h"
//BlankLine

/*@+NonDeterm@*/ 
static __global__ void init(data_type* const priority, flag_t* const status, flag_t* const lost, const int size) 
/*@-NonDeterm@*/ 
/*@+Determ@*/ 
static __global__ void init(data_type* const priority, flag_t* const status, flag_t* const status_n, flag_t* const lost, const int size) 
/*@-Determ@*/ 
{
  // initialize arrays
  const int v = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  if (v < size)
  {
    /*@IntType@*/ priority[v] = hash(v + 712313887); /*@LongType@*/ priority[v] = ((unsigned long)hash(v + 712313887)) | ((unsigned long)hash(v + 683067839) << (sizeof (unsigned int) * 8));
    /*@NoUninitializedBug@*/ status[v] = undecided; /*@UninitializedBug@*/
    /*@NonDeterm@*/ /*@Determ@*/ status_n[v] = undecided;
    lost[v] = 0;
  }
}
//BlankLine

/*@+NonDeterm@*/ 
static __global__ void mis(const ECLgraph g, const int* const sp, const data_type* const priority, flag_t* const status, flag_t* const lost) 
/*@-NonDeterm@*/ 
/*@+Determ@*/ 
static __global__ void mis(const ECLgraph g, const int* const sp, const data_type* const priority, flag_t* const status, flag_t* const status_n, flag_t* const lost)
/*@-Determ@*/
{
  /*@NoFieldBug@*/ const int N = g.edges; /*@FieldBug@*/ const int N = g.nodes;
  
  // go over all edges
  /*@+NonPersist@*/
  int e = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  /*@NoExcessThreadsBug@*/ if (e < N) { /*@ExcessThreadsBug@*/ { /*@NoBoundsBug@*/ suppress /*@BoundsBug@*/ suppress
  /*@-NonPersist@*/

  /*@+Persist@*/ /*@+NoBoundsBug@*/
  int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  for (int e = tid; e < N; e += gridDim.x * ThreadsPerBlock) {
  /*@-Persist@*/ /*@-NoBoundsBug@*/
  
  /*@+Persist@*/ /*@+BoundsBug@*/
  int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  for (int e = tid; e <= N; e += gridDim.x * ThreadsPerBlock) {
  /*@-Persist@*/ /*@-BoundsBug@*/

    //BlankLine
    const int src = sp[e];
    const int dst = g.nlist[e];
    /*@Atomic@*/ const int srcStatus = atomicRead(&status[src]); /*@CudaAtomic@*/ const int srcStatus = status[src].load(); /*@RaceBug@*/ const int srcStatus = status[src];
    /*@Atomic@*/ const int dstStatus = atomicRead(&status[dst]); /*@CudaAtomic@*/ const int dstStatus = status[dst].load(); /*@RaceBug@*/ const int dstStatus = status[dst];
    //BlankLine
    
    // if one is included, exclude the other
    if (srcStatus == included) {
      /*@+NonDeterm@*/
      /*@Atomic@*/ atomicWrite(&status[dst], excluded); /*@CudaAtomic@*/ status[dst].store(excluded); /*@RaceBug@*/ status[dst] = excluded;
      /*@-NonDeterm@*/
      /*@+Determ@*/
      /*@Atomic@*/ atomicWrite(&status_n[dst], excluded); /*@CudaAtomic@*/ status_n[dst].store(excluded); /*@RaceBug@*/ suppress
      /*@-Determ@*/
    }
    else if (dstStatus == included) {
      /*@+NonDeterm@*/
      /*@Atomic@*/ atomicWrite(&status[src], excluded); /*@CudaAtomic@*/ status[src].store(excluded); /*@RaceBug@*/ status[src] = excluded;
      /*@-NonDeterm@*/
      /*@+Determ@*/
      /*@Atomic@*/ atomicWrite(&status_n[src], excluded); /*@CudaAtomic@*/ status_n[src].store(excluded); /*@RaceBug@*/ suppress
      /*@-Determ@*/
    } else if (srcStatus == undecided && dstStatus == undecided) {
    // if both undecided -> mark lower as lost
      if (priority[src] < priority[dst]) {
          /*@Atomic@*/ atomicWrite(&lost[src], 1); /*@CudaAtomic@*/ lost[src].store(1); /*@RaceBug@*/ lost[src] = 1;
      } else {
          /*@Atomic@*/ atomicWrite(&lost[dst], 1); /*@CudaAtomic@*/ lost[dst].store(1); /*@RaceBug@*/ lost[dst] = 1;
      }
    }
  }
}
//BlankLine

/*@+NonDeterm@*/ 
static __global__ void mis_vertex_pass(const ECLgraph g, const int* const sp, flag_t* const status, flag_t* const lost, flag_t* const goagain)
/*@-NonDeterm@*/ 
/*@+Determ@*/ 
static __global__ void mis_vertex_pass(const ECLgraph g, const int* const sp, flag_t* const status, flag_t* const status_n, flag_t* const lost, flag_t* const goagain)
/*@-Determ@*/
{
  /*@NoFieldBug@*/ const int N = g.edges; /*@FieldBug@*/ const int N = g.nodes;
  
  // go over all edges
  /*@+NonPersist@*/
  int e = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  /*@NoExcessThreadsBug@*/ if (e < N) { /*@ExcessThreadsBug@*/ { /*@NoBoundsBug@*/ suppress /*@BoundsBug@*/ suppress
  /*@-NonPersist@*/

  /*@+Persist@*/ /*@+NoBoundsBug@*/
  int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  for (int e = tid; e < N; e += gridDim.x * ThreadsPerBlock) {
  /*@-Persist@*/ /*@-NoBoundsBug@*/
  
  /*@+Persist@*/ /*@+BoundsBug@*/
  int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  for (int e = tid; e <= N; e += gridDim.x * ThreadsPerBlock) {
  /*@-Persist@*/ /*@-BoundsBug@*/

    //BlankLine
    const int src = sp[e];
    const int dst = g.nlist[e];
    /*@Atomic@*/ const int srcStatus = atomicRead(&status[src]); /*@CudaAtomic@*/ const int srcStatus = status[src].load(); /*@RaceBug@*/ const int srcStatus = status[src];
    /*@Atomic@*/ const int dstStatus = atomicRead(&status[dst]); /*@CudaAtomic@*/ const int dstStatus = status[dst].load(); /*@RaceBug@*/ const int dstStatus = status[dst];
    //BlankLine
  
    // if v didn't lose
    // if src won
    if (lost[src] == 0) {
      if (srcStatus == undecided) {
      // and is undecided -> include
        /*@+NonDeterm@*/
        /*@Atomic@*/ atomicWrite(&status[src], included); /*@CudaAtomic@*/ status[src].store(included); /*@RaceBug@*/ status[src] = included;
        /*@-NonDeterm@*/
        /*@+Determ@*/
        /*@Atomic@*/ atomicWrite(&status_n[src], included); /*@CudaAtomic@*/ status_n[src].store(included); /*@RaceBug@*/ suppress
        /*@-Determ@*/
      }
    }
    // if dst won
    if (lost[dst] == 0) {
      if (dstStatus == undecided) {
      // and is undecided -> include
        /*@+NonDeterm@*/
        /*@Atomic@*/ atomicWrite(&status[dst], included); /*@CudaAtomic@*/ status[dst].store(included); /*@RaceBug@*/ status[dst] = included;
        /*@-NonDeterm@*/
        /*@+Determ@*/
        /*@Atomic@*/ atomicWrite(&status_n[dst], included); /*@CudaAtomic@*/ status_n[dst].store(included); /*@RaceBug@*/ suppress
        /*@-Determ@*/
      }
    }
    // if either is still undecided, goagain
    if (srcStatus == undecided || dstStatus == undecided) {
      /*@Atomic@*/ atomicWrite(goagain, 1); /*@CudaAtomic@*/ *goagain = 1; /*@RaceBug@*/ *goagain = 1;
    }
  }
}
//BlankLine

static __global__ void mis_last_pass(flag_t* const status, const int size)
{
  /*@+NonPersist@*/
  int w = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  if (w < size) {
  /*@-NonPersist@*/

  /*@+Persist@*/
  int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock;
  for (int w = tid; w < size; w += gridDim.x * ThreadsPerBlock) {
  /*@-Persist@*/
  
    if (status[w] == undecided)
    {
      status[w] = included;
    }
  }
}
//BlankLine

static double GPUmis_edge(const ECLgraph g, const int* const sp, data_type* const priority, int* const status)
{
  flag_t* d_goagain;
  if (cudaSuccess != cudaMalloc((void **)&d_goagain, sizeof(flag_t))) fprintf(stderr, "ERROR: could not allocate d_goagain\n");
  data_type* d_priority;
  if (cudaSuccess != cudaMalloc((void **)&d_priority, g.nodes * sizeof(data_type))) fprintf(stderr, "ERROR: could not allocate d_priority\n");
  flag_t* d_status;
  if (cudaSuccess != cudaMalloc((void **)&d_status, g.nodes * sizeof(flag_t))) fprintf(stderr, "ERROR: could not allocate d_status\n");
  flag_t* d_lost;
  if (cudaSuccess != cudaMalloc((void **)&d_lost, g.nodes * sizeof(flag_t))) fprintf(stderr, "ERROR: could not allocate d_lost\n");
  /*@+Determ@*/
  flag_t* d_status_new;
  if (cudaSuccess != cudaMalloc((void **)&d_status_new, g.nodes * sizeof(flag_t))) fprintf(stderr, "ERROR: could not allocate d_status_new\n");
  /*@-Determ@*/
  //BlankLine

  /*@+NonPersist@*/
  int blocks = (g.edges + ThreadsPerBlock - 1) / ThreadsPerBlock;
  /*@-NonPersist@*/
  /*@+Persist@*/
  const int ThreadsBound = GPUinfo(0, false);
  const int blocks = ThreadsBound / ThreadsPerBlock;
  /*@-Persist@*/
  //BlankLine
  
  /*@NonDeterm@*/ init<<<(g.edges + ThreadsPerBlock - 1) / ThreadsPerBlock, ThreadsPerBlock>>>(d_priority, d_status, d_lost, g.nodes); /*@Determ@*/ init<<<(g.edges + ThreadsPerBlock - 1) / ThreadsPerBlock, ThreadsPerBlock>>>(d_priority, d_status, d_status_new, d_lost, g.nodes);
  
  //BlankLine
  struct timeval beg, end;
  gettimeofday(&beg, NULL);
  //BlankLine

  flag_t goagain;
  int iter = 0;
  do {
    iter++;
    cudaMemset(d_goagain, 0, sizeof(flag_t));
    //BlankLine
    
    // edge pass
    /*@+NonDeterm@*/
    mis<<<blocks, ThreadsPerBlock>>>(g, sp, d_priority, d_status, d_lost); 
    /*@-NonDeterm@*/
    /*@+Determ@*/
    mis<<<blocks, ThreadsPerBlock>>>(g, sp, d_priority, d_status, d_status_new, d_lost);
    /*@-Determ@*/
    //BlankLine
    
    /*@NonDeterm@*/ /*@Determ@*/ if (cudaSuccess != cudaMemcpy(d_status, d_status_new, g.nodes * sizeof(flag_t), cudaMemcpyDeviceToDevice)) fprintf(stderr, "ERROR: copying of d_status_new to d_status on device failed\n");
    
    // vertex pass
    /*@+NonDeterm@*/
    mis_vertex_pass<<<blocks, ThreadsPerBlock>>>(g, sp, d_status, d_lost, d_goagain); 
    /*@-NonDeterm@*/
    /*@+Determ@*/
    mis_vertex_pass<<<blocks, ThreadsPerBlock>>>(g, sp, d_status, d_status_new, d_lost, d_goagain);
    /*@-Determ@*/
    //BlankLine
    
    cudaMemset(d_lost, 0, g.nodes * sizeof(flag_t));
    if (cudaSuccess != cudaMemcpy(&goagain, d_goagain, sizeof(int), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of go_again from device failed\n");
    /*@NonDeterm@*/ /*@Determ@*/ if (cudaSuccess != cudaMemcpy(d_status, d_status_new, g.nodes * sizeof(flag_t), cudaMemcpyDeviceToDevice)) fprintf(stderr, "ERROR: copying of d_status_new to d_status on device failed\n");
  } while (goagain);

  //BlankLine
  /*@+NonPersist@*/
  blocks = (g.nodes + ThreadsPerBlock - 1) / ThreadsPerBlock;
  /*@-NonPersist@*/
  // include all remaining nodes that have no edges
  mis_last_pass<<<blocks, ThreadsPerBlock>>>(d_status, g.nodes); 
  
  //BlankLine
  cudaDeviceSynchronize();
  gettimeofday(&end, NULL);
  const double runtime = end.tv_sec - beg.tv_sec + (end.tv_usec - beg.tv_usec) / 1000000.0;
  //BlankLine
  
  CheckCuda();
  if (cudaSuccess != cudaMemcpy(status, d_status, g.nodes * sizeof(flag_t), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of status from device failed\n"); 
  //BlankLine

  // determine and print set size
  int cnt = 0;
  for (int v = 0; v < g.nodes; v++) {
    if (status[v] == included) cnt++;
  }
  printf("iterations: %d,  elements in set: %d (%.1f%%)\n", iter, cnt, 100.0 * cnt / g.nodes);
  //BlankLine

  /*@NonDeterm@*/ /*@Determ@*/ cudaFree(d_status_new);
  cudaFree(d_status);
  cudaFree(d_priority);
  cudaFree(d_lost);
  cudaFree(d_goagain);
  return runtime;
}
