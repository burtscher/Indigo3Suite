
/*@FloatType@*/ typedef float data_type; /*@DoubleType@*/ typedef double data_type;
typedef data_type basic_t;
/*@NoFieldBug@*/ declare /*@FieldBug@*/ declare
/*@NoPrecedenceBug@*/ declare /*@PrecedenceBug@*/ declare
/*@NoExcessThreadsBug@*/ declare /*@ExcessThreadsBug@*/ declare /*@NoBoundsBug@*/ declare /*@BoundsBug@*/ declare
/*@NoNbrBoundsBug@*/ declare /*@NbrBoundsBug@*/ declare
/*@NoSyncBug@*/ declare /*@SyncBug@*/ declare

/*@+Persist@*/
/*@NoExcessThreadsBug@*/ suppress /*@ExcessThreadsBug@*/ suppress /*@NoBoundsBug@*/ /*@BoundsBug@*/ 
/*@NoPrecedenceBug@*/ /*@PrecedenceBug@*/ suppress
/*@-Persist@*/

/*@+GlobalAdd@*/ 
/*@NoSyncBug@*/ /*@SyncBug@*/ suppress 
/*@-GlobalAdd@*/ 
static const int ThreadsPerBlock = 512;
#include "indigo_pr_cuda.h"

/*@NonPersist@*/ declare /*@Persist@*/ declare

//BlankLine
__global__ void contrib(int nodes, data_type* scores, int* degree, data_type* outgoing_contrib, data_type* incoming_total)
{
  /*@+NonPersist@*/
  /*@Thread@*/ int src = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int src = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int src = blockIdx.x;
  if (src < nodes) {
  /*@-NonPersist@*/

  /*@+Persist@*/
  /*@Thread@*/ int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int tid = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int tid = blockIdx.x;
  /*@-Persist@*/

  /*@+Persist@*/
  /*@Thread@*/ for (int src = tid; src < nodes; src += gridDim.x * ThreadsPerBlock) { /*@Warp@*/ for (int src = tid; src < nodes; src += gridDim.x * (ThreadsPerBlock / WarpSize)) { /*@Block@*/ for (int src = tid; src < nodes; src += gridDim.x) {
  /*@-Persist@*/
    outgoing_contrib[src] = scores[src] / degree[src];
    incoming_total[src] = 0;
  }
}
//BlankLine
__global__ void push(int nodes, const int* const __restrict__ nindex, const int* const __restrict__ nlist, data_type* outgoing_contrib, data_type* incoming_total)
{
  /*@+NonPersist@*/
  /*@Thread@*/ int src = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int src = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int src = blockIdx.x;
  /*@NoExcessThreadsBug@*/ if (src < nodes) { /*@ExcessThreadsBug@*/ { /*@NoBoundsBug@*/ suppress /*@BoundsBug@*/ suppress
  /*@-NonPersist@*/

  /*@+Persist@*/
  /*@Thread@*/ int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int tid = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int tid = blockIdx.x;
  /*@-Persist@*/

  /*@+Persist@*/ /*@+NoBoundsBug@*/ 
  /*@Thread@*/ for (int src = tid; src < nodes; src += gridDim.x * ThreadsPerBlock) { /*@Warp@*/ for (int src = tid; src < nodes; src += gridDim.x * (ThreadsPerBlock / WarpSize)) { /*@Block@*/ for (int src = tid; src < nodes; src += gridDim.x) {
  /*@-Persist@*/ /*@-NoBoundsBug@*/ 

  /*@+Persist@*/ /*@+BoundsBug@*/ 
  /*@Thread@*/ for (int src = tid; src <= nodes; src += gridDim.x * ThreadsPerBlock) { /*@Warp@*/ for (int src = tid; src <= nodes; src += gridDim.x * (ThreadsPerBlock / WarpSize)) { /*@Block@*/ for (int src = tid; src <= nodes; src += gridDim.x) {
  /*@-Persist@*/ /*@-BoundsBug@*/ 

    const int beg = nindex[src];
    const int end = nindex[src + 1];
    const data_type outgoing = outgoing_contrib[src];

    // iterate neighbor list
    /*@+NoNbrBoundsBug@*/ 
    /*@Thread@*/ for (int i = beg; i < end; i++) { /*@Warp@*/ for (int i = beg + threadIdx.x % WarpSize; i < end; i += WarpSize) { /*@Block@*/ for (int i = beg + threadIdx.x; i < end; i += ThreadsPerBlock) {
    /*@-NoNbrBoundsBug@*/ 

    /*@+NbrBoundsBug@*/ 
    /*@Thread@*/ for (int i = beg; i <= end; i++) { /*@Warp@*/ for (int i = beg + threadIdx.x % WarpSize; i <= end; i += WarpSize) { /*@Block@*/ for (int i = beg + threadIdx.x; i <= end; i += ThreadsPerBlock) {
    /*@-NbrBoundsBug@*/ 

      int dst = nlist[i];
      /*@NoRaceBug@*/ atomicAdd(&incoming_total[dst], outgoing); /*@RaceBug@*/ incoming_total[dst] += outgoing;
    }
  }
}
//BlankLine
__global__ void compute(const ECLgraph g, data_type* scores, data_type* diff, data_type base_score, data_type* incoming_total)
{
  /*@NoFieldBug@*/ const int N = g.nodes; /*@FieldBug@*/ const int N = g.edges;
  /*@+Reduction@*/
  __shared__ data_type buffer[WarpSize];
  /*@-Reduction@*/

  /*@+BlockAdd@*/
  __shared__ data_type local_diff;
  if (threadIdx.x == 0) local_diff = 0;
  /*@NoSyncBug@*/ __syncthreads(); /*@SyncBug@*/
  /*@-BlockAdd@*/
  data_type error = 0;
  /*@+NonPersist@*/
  /*@Thread@*/ int src = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int src = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int src = blockIdx.x;
  if (src < N) {
  /*@-NonPersist@*/

  /*@+Persist@*/
  /*@Thread@*/ int tid = threadIdx.x + blockIdx.x * ThreadsPerBlock; /*@Warp@*/ int tid = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / WarpSize; /*@Block@*/ int tid = blockIdx.x;
  /*@-Persist@*/

  /*@+Persist@*/
  /*@Thread@*/ for (int src = tid; src < N; src += gridDim.x * ThreadsPerBlock) { /*@Warp@*/ for (int src = tid; src < N; src += gridDim.x * (ThreadsPerBlock / WarpSize)) { /*@Block@*/ for (int src = tid; src < N; src += gridDim.x) {
  /*@-Persist@*/
    data_type old_score = scores[src];
    data_type incoming = incoming_total[src];
    const data_type value = base_score + kDamp * incoming;
    scores[src] = value;
    error = fabs(value - old_score);
    /*@GlobalAdd@*/ atomicAdd(diff, error); /*@BlockAdd@*/ atomicAdd_block(&local_diff, error); /*@Reduction@*/ error = block_sum_reduction(error, buffer);
    /*@+Reduction@*/
    /*@NoSyncBug@*/ __syncthreads(); /*@SyncBug@*/
    if (threadIdx.x == 0) atomicAdd(diff, error);
    /*@-Reduction@*/
  }
  /*@+BlockAdd@*/
  /*@NoSyncBug@*/ __syncthreads(); /*@SyncBug@*/
  if (threadIdx.x == 0) atomicAdd(diff, local_diff);
  /*@-BlockAdd@*/
}
//BlankLine
double PR_GPU(const ECLgraph g, data_type *scores, int* degree)
{
  // declare device variables
  ECLgraph d_g = g;
  int *d_degree;
  data_type *d_scores, *d_sums, *d_contrib, *d_incoming;
  data_type *d_diff, h_diff;

  /*@+Persist@*/
  const int ThreadsBound = GPUinfo(0);
  const int blocks = ThreadsBound / ThreadsPerBlock;
  /*@-Persist@*/

  // allocate device memory
  cudaMalloc((void **)&d_degree, g.nodes * sizeof(int));
  cudaMalloc((void **)&d_scores, g.nodes * sizeof(data_type));
  cudaMalloc((void **)&d_sums, g.nodes * sizeof(data_type));
  cudaMalloc((void **)&d_contrib, g.nodes * sizeof(data_type));
  cudaMalloc((void **)&d_incoming, g.nodes * sizeof(data_type));
  cudaMalloc((void **)&d_diff, sizeof(data_type));
  if (cudaSuccess != cudaMalloc((void **)&d_g.nindex, (g.nodes + 1) * sizeof(int))) fprintf(stderr, "ERROR: could not allocate nindex\n");
  if (cudaSuccess != cudaMalloc((void **)&d_g.nlist, g.edges * sizeof(int))) fprintf(stderr, "ERROR: could not allocate nlist\n");

  // copy data to device
  cudaMemcpy(d_degree, degree, g.nodes * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(d_scores, scores, g.nodes * sizeof(data_type), cudaMemcpyHostToDevice);
  if (cudaSuccess != cudaMemcpy(d_g.nindex, g.nindex, (g.nodes + 1) * sizeof(int), cudaMemcpyHostToDevice)) fprintf(stderr, "ERROR: copying of index to device failed\n");
  if (cudaSuccess != cudaMemcpy(d_g.nlist, g.nlist, g.edges * sizeof(int), cudaMemcpyHostToDevice)) fprintf(stderr, "ERROR: copying of nlist to device failed\n");

  /*@+NonPersist@*/ /*@+NoPrecedenceBug@*/
  /*@Thread@*/ const int blocks = (g.nodes + ThreadsPerBlock - 1) / ThreadsPerBlock; /*@Warp@*/ const unsigned int blocks = ((unsigned int)g.nodes * (unsigned int)WarpSize + (unsigned int)ThreadsPerBlock - 1) / (unsigned int)ThreadsPerBlock; /*@Block@*/ const int blocks = ((unsigned int)g.nodes * (unsigned int)ThreadsPerBlock + (unsigned int)ThreadsPerBlock - 1) / (unsigned int)ThreadsPerBlock;
  /*@-NonPersist@*/ /*@-NoPrecedenceBug@*/

  /*@+NonPersist@*/ /*@+PrecedenceBug@*/
  /*@Thread@*/ suppress /*@Warp@*/ const unsigned int blocks = (g.nodes * WarpSize + ThreadsPerBlock - 1) / ThreadsPerBlock; /*@Block@*/ const int blocks = (g.nodes * ThreadsPerBlock + ThreadsPerBlock - 1) / ThreadsPerBlock;
  /*@-NonPersist@*/ /*@-PrecedenceBug@*/

  const data_type base_score = (1.0f - kDamp) / (data_type)g.nodes;

  // timer
  struct timeval start, end;
  int iter = 0;

  gettimeofday(&start, NULL);
  do {
    iter++;
    h_diff = 0;
    if (cudaSuccess != cudaMemcpy(d_diff, &h_diff, sizeof(data_type), cudaMemcpyHostToDevice)) fprintf(stderr, "ERROR: copying of h_diff to device failed\n");
    contrib<<<blocks, ThreadsPerBlock>>>(g.nodes, d_scores, d_degree, d_contrib, d_incoming);
    push<<<blocks, ThreadsPerBlock>>>(g.nodes, d_g.nindex, d_g.nlist, d_contrib, d_incoming);

    compute<<<blocks, ThreadsPerBlock>>>(d_g, d_scores, d_diff, base_score, d_incoming);
    if (cudaSuccess != cudaMemcpy(&h_diff, d_diff, sizeof(data_type), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of d_diff from device failed\n");
  } while (h_diff > EPSILON && iter < MAX_ITER);
  cudaDeviceSynchronize();
  gettimeofday(&end, NULL);

  if (iter < MAX_ITER) iter++;
  const double runtime = end.tv_sec + end.tv_usec / 1000000.0 - start.tv_sec - start.tv_usec / 1000000.0;
  printf("GPU iterations = %d.\n", iter);

  if (cudaSuccess != cudaMemcpy(scores, d_scores, g.nodes * sizeof(data_type), cudaMemcpyDeviceToHost)) fprintf(stderr, "ERROR: copying of d_scores from device failed\n");

  cudaFree(d_degree);
  cudaFree(d_scores);
  cudaFree(d_sums);
  cudaFree(d_contrib);
  cudaFree(d_diff);
  return runtime;
}
